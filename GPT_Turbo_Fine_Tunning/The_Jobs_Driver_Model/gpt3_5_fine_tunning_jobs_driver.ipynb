{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the API key and necessary variables \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import config, os, openai, tiktoken, json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = config.OPENAI_API_KEY\n",
    "os.environ['ORGANIZATION_ID'] = config.ORG_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_jobs_driver.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Divide the dataset between questions and answers\n",
    "conversations = content.strip().split('\\nQ: ')\n",
    "\n",
    "# Add \"user:\" y \"assistant:\" in each questions\n",
    "formatted_content = ''\n",
    "for conversation in conversations:\n",
    "    parts = conversation.split('\\nA: ')\n",
    "    if len(parts) == 2:\n",
    "        formatted_content += f\"user: {parts[0]}\\nassistant: {parts[1]}\\n-\\n\"\n",
    "    else:\n",
    "        formatted_content += conversation + '\\n'\n",
    "\n",
    "# Save the new one\n",
    "with open('dataset_jobs_driver_refactor.txt', 'w') as file:\n",
    "    file.write(formatted_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the dataset\n",
    "with open('dataset_jobs_driver_refactor.txt', encoding='latin-1') as f:\n",
    "    text = [line for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"user: Q: Will you continue running my campaign even if it doesn't reach the minimum target number of leads? \\n\",\n",
       " 'assistant: Yes, we will continue running the campaign until we reach the minimum target number of leads, or we can issue a pro-rated credit.\\n',\n",
       " '-\\n',\n",
       " \"user: Will you continue running my campaign even if it doesn't reach the minimum target number of leads? \\n\",\n",
       " \"assistant: The LATS is a convenient tool for clients who don't have an ATS or prefer a more straightforward solution. It functions as a centralized hub where clients can view and manage all of their leads in one place, making it easy for them to track their job search efforts and follow up with interested candidates. With our LATS, clients can experience the benefits of an ATS without the added complexity, making the job search process more streamlined and efficient.\\n\",\n",
       " '-\\n',\n",
       " \"user: What is TheJobsDriver's client retention rate? \\n\",\n",
       " \"assistant: The exact same thing. So, how do you stand out and get the best candidates? By going beyond traditional methods and using TheJobsDriver. Our algorithm and human-powered tools allow us to not only hit the places where active job seekers are looking but also to reach the 70% of people who aren't actively looking for a job - the passive job seekers who may be perfect for your open position and are not being called by your competitors. By using TheJobsDriver, you'll save not only time and resources but also get a broader and more diverse pool of high-quality candidates.\\n\",\n",
       " '-\\n',\n",
       " 'user: On which platforms does TheJobsDriver advertise? \\n']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We see how difference each example in the dataset\n",
    "text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to ensure that we follow the next format, according to Open IA documentation\n",
    "```\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are an assistant that occasionally misspells words\" },\n",
    "    { \"role\": \"user\", \"content\": \"Tell me a story.\" },\n",
    "    { \"role\": \"assistant\", \"content\": \"One day a student went to schoool.\" }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to refactor each example as the properly format\n",
    "def refactor_example(example_list, system_message=None):\n",
    "    messages = []\n",
    "\n",
    "    # Include the general system message\n",
    "    if system_message:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_message\n",
    "        })\n",
    "\n",
    "    # Iterate for each example\n",
    "    for message in example_list:\n",
    "        example = message.split(': ', maxsplit=1)\n",
    "\n",
    "        #Control if each example dont follow the format\n",
    "        if len(example) < 2:\n",
    "            continue\n",
    "\n",
    "        role = example[0].strip()\n",
    "        content = example[1].strip()\n",
    "\n",
    "        message_format = {\n",
    "            \"role\": role,\n",
    "            \"content\": content\n",
    "        }\n",
    "\n",
    "        messages.append(message_format)\n",
    "\n",
    "    dict_final = {\n",
    "        \"messages\": messages\n",
    "    }\n",
    "\n",
    "    return dict_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply the function in each example\n",
    "system_message = \"As a representative of TheJobsDriver, your role is to prioritize friendly, \\\n",
    "elevated conversation while anticipating that questioners may challenge your points. \\\n",
    "Approach these conversations like a game of chess, not checkers, in your responses emphasize generating high-quality leads while \\\n",
    "adhering to our proven processes. Highlight \\\n",
    "TheJobsDrivers distinctive approach and competitive edge. Emphasize delivering high-quality leads to clients \\\n",
    "through a single KPI, the number of interested job seekers.\"\n",
    "\n",
    "dataset = []\n",
    "\n",
    "example = []\n",
    "for line in text:\n",
    "  if line == '-\\n':\n",
    "    format_example = refactor_example(example_list=example,\n",
    "                                            system_message=system_message)\n",
    "\n",
    "    dataset.append(format_example)\n",
    "    example = []\n",
    "    continue\n",
    "\n",
    "  example.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'As a representative of TheJobsDriver, your role is to prioritize friendly, elevated conversation while anticipating that questioners may challenge your points. Approach these conversations like a game of chess, not checkers, in your responses. Emphasize generating high-quality leads while adhering to our proven processes. Highlight TheJobsDrivers distinctive approach and competitive edge. Emphasize delivering high-quality leads to clients through a single KPI, the number of interested job seekers.'},\n",
       "  {'role': 'user', 'content': 'What can TheJobsDriver for me?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'is able to offer a comprehensive and effective solution for companies facing challenges in filling hard-to-find positions or experiencing high turnover with the help of a wide range of productive media sources for recruiting, including social networks, search engines, digital billboards, news sources and even programmatic audio to generate leads for our clients'}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check for errors and estimate the price using the guide [provided by OpenAI](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "            print(message)\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "            print(message)\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            print(message)\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "            print(message)\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "        print(message)\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate the number of tokens and a aproximate of the cost of this fine tunning, based on [OpenIA GitHub](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"average / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Distribution of Number of messages for example:\n",
      "min / max: 3, 3\n",
      "average / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of Number of token for example:\n",
      "min / max: 127, 422\n",
      "average / median: 185.95108695652175, 154.5\n",
      "p5 / p95: 136.0, 281.40000000000003\n",
      "\n",
      "#### Distribution of Number of tokens by assistant for example:\n",
      "min / max: 11, 289\n",
      "average / median: 66.1195652173913, 34.0\n",
      "p5 / p95: 18.0, 157.40000000000003\n",
      "\n",
      "0 examples that exceed the token limit of 4096, they will be truncated during fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts by each type of user\n",
    "n_messages, convo_lens, assistant_message_lens = [], [], []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print_distribution(n_messages, \"Number of messages for example\")\n",
    "print_distribution(convo_lens, \"Number of token for example\")\n",
    "print_distribution(assistant_message_lens, \"Number of tokens by assistant for example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples that exceed the token limit of 4096, they will be truncated during fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset have ~34215 tokens to use in the training phase\n",
      "By default, we will use 4 epochs in this training\n",
      "So, will be ~136860 tokens in total\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "TARGET_EPOCHS = 4\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"The dataset have ~{n_billing_tokens_in_dataset} tokens to use in the training phase\")\n",
    "print(f\"By default, we will use {n_epochs} epochs in this training\")\n",
    "print(f\"So, will be ~{n_epochs * n_billing_tokens_in_dataset} tokens in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into account this official pricing in [Open IA Fine Tunnig - Fees](https://openai.com/pricing)\n",
    "\n",
    "| Model         | Training            | Input usage         | Output usage        |   \n",
    "|---------------|---------------------|---------------------|---------------------|\n",
    "| GPT-3.5 Turbo | $0.0080 / 1K tokens | $0.0120 / 1K tokens | $0.0160 / 1K tokens |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate cost of the training is ~$1.09\n"
     ]
    }
   ],
   "source": [
    "training_cost = '${:,.2f}'.format(((n_epochs * n_billing_tokens_in_dataset)/1000)*0.008)\n",
    "print(f'The estimate cost of the training is ~{training_cost}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the dataset in the properly format (json)\n",
    "with open('jobs_driver_train_full.jsonl', 'w') as file:\n",
    "    for ejemplo in dataset:\n",
    "        json_line = json.dumps(ejemplo)\n",
    "        file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tunning Job in Open IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.organization = os.getenv(\"ORGANIZATION_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: file-QIqUQ2qXks5UOkKTA3sHS6Ca\n"
     ]
    }
   ],
   "source": [
    "# We save the dataset and we save the id\n",
    "train_full_response_file = openai.File.create(\n",
    "    file=open('jobs_driver_train_full.jsonl','rb'),\n",
    "    purpose='fine-tune'\n",
    ")\n",
    "\n",
    "\n",
    "print(f'id: {train_full_response_file.id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jobs_driver_train_full.jsonl', 'r') as file:\n",
    "    a = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.FineTuningJob.create(training_file=train_full_response_file.id,\n",
    "                                       model=\"gpt-3.5-turbo\",\n",
    "                                       suffix='jobs_driver_v2',\n",
    "                                       hyperparameters={'n_epochs':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ftjob-ixU6IoY60DwyPvzhLUz6CVbA'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_job = response.id\n",
    "id_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 441/620: training loss=0.75\n",
      "Step 451/620: training loss=0.83\n",
      "Step 461/620: training loss=1.05\n",
      "Step 471/620: training loss=0.89\n",
      "Step 481/620: training loss=0.31\n",
      "Step 491/620: training loss=0.42\n",
      "Step 501/620: training loss=2.94\n",
      "Step 511/620: training loss=0.45\n",
      "Step 521/620: training loss=1.58\n",
      "Step 531/620: training loss=0.06\n",
      "Step 541/620: training loss=1.52\n",
      "Step 551/620: training loss=1.41\n",
      "Step 561/620: training loss=1.84\n",
      "Step 571/620: training loss=0.49\n",
      "Step 581/620: training loss=0.44\n",
      "Step 591/620: training loss=0.85\n",
      "Step 601/620: training loss=0.85\n",
      "Step 611/620: training loss=0.66\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0613:thejobsdriver:english-teacher:8EgXprcU\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "response = openai.FineTuningJob.list_events(id=response.id)\n",
    "\n",
    "events = response[\"data\"]\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event[\"message\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
